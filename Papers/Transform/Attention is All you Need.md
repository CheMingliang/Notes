# Attention is All you Need

- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin
- NIPS 2017
- [论文](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)

#### 总结
提出了完全基于注意力机制的序列转换模型，用多头注意力自注意力取代了编码器-解码器架构中最常见的循环或卷积结构

#### 背景


#### 方法


#### 要点
自注意力（有时称为内部注意力）是一种将单个序列的不同位置相关联的注意力机制，以便计算序列的表示

编码器将符号表示的输入序列 $(x_1,...,x_n)$ 映射到连续表示序列 $z=(z_1,...,z_n)$。给定z，解码器每次生成一个元素的符号输出序列 $(y_1,...,y_m)$

添加掩码修改解码器中的自注意力子层，掩码与编码输出偏移一个位置，来防止当前位置关注后续位置，保证位置 i 的预测只能依赖于小于 i 的位置处的已知输出

多头注意力三种不同的使用方式：
- 在编码器-解码器注意层中，查询来自先前的解码器层，并且保存来自编码器输出的键和值
- 编码器包含自注意力层
- 解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包含该位置的所有位置

使用正弦位置生成，因为它可以允许模型判断出比训练期间遇到的序列长度更长的序列长度

将三个自注意力机制用于三个需求：
- 每层的总计算复杂度
- 可以并行化的计算量，以所需的最小顺序操作数来衡量
- 网络中远程依赖之间的路径长度

作为附带好处，自注意力可以产生更多可解释性模型

#### 思考