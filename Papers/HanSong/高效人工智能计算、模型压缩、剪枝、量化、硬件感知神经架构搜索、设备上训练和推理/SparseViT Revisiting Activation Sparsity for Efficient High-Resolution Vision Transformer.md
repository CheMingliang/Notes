# SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer

- Xuanyao Chen, Zhijian Liu4, Haotian Tang, Li Yi, Hang Zhao, Song Han
- CVPR 2023
- [论文](http://arxiv.org/abs/2303.17605)
#### 总结


#### 背景
高分辨率图像使神经网络能够学习更丰富的视觉表征，高分辨率带来性能提高的同时增加了计算复杂度，这不利于神经网络在对延迟敏感的应用中使用

**一个基本观察是：图像中包含详细物体特征的像素比背景像素更重要**

利用基本观察，跳过对不太重要区域的计算是减少计算量的一种简单而有效的方法，但是跳过的同时打破了密集卷积工作量的规律性，因此很难将其转化为实际速度提升

一种模型压缩和加速技术是神经网络剪枝，删除不同粒度级别的冗余权重，一般需要专门的硬件设计才能转化为实际速度提升

另一种激活剪枝侧重于降低训练期间的内存成本，很少有改善推理延迟的方法，同时也需要相应的系统支持

#### 方法
**基本观察：图像中包含详细物体特征的像素比背景像素更重要**




#### 要点


#### 问题
P3：后面的层更容易修剪，因为更接近输出


#### 思考
P3 Shared Scoring 公用一个窗口重要性，是否可以优化